# Windows6.1-KB2462137-v2-x64 update.mum

## DirectoryServices-ADAM-Package-Client [目录服务 ADAM 包 客户端]

### What is ADAM ?
   
    Adam is a first-order optimization algorithm that can replace the traditional stochastic gradient descent process, which iteratively updates the neural network weights based on the training data. Adam was originally proposed by Diederik Kingma of OpenAI and Jimmy Ba of the University of Toronto in the ICLR paper submitted to 2015 (Adam: A Method for Stochastic Optimization). Both parts of this paper are based on the discussion and interpretation of the paper.

    First of all, the algorithm is called "Adam", which is not an acronym or a person's name. Its name comes from adaptive moment estimation. When introducing this algorithm, the original paper listed the advantages of applying the Adam optimization algorithm to non-convex optimization problems:

    straight to the point
    Efficient computing
    less memory required
    Invariance to diagonal scaling of gradients (proved in part 2)
    Suitable for solving optimization problems with large-scale data and parameters
    Suitable for non-stationary targets
    Suitable for problems with very noisy or sparse gradients
    Hyperparameters can be interpreted intuitively, and basically only need a small amount of tuning
